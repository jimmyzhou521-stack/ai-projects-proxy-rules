#!/usr/bin/env python3
"""
é‡‡é›†çƒ­é—¨AIç½‘ç«™å’Œé¡¹ç›®
Collect popular AI websites and projects
"""

import json
import re
import requests
from datetime import datetime
from typing import List, Dict, Set

# å†…ç½®çƒ­é—¨AIæœåŠ¡åŸŸååˆ—è¡¨
BUILT_IN_AI_DOMAINS = [
    # AIèŠå¤©å’Œå¯¹è¯
    "openai.com",
    "chat.openai.com",
    "platform.openai.com",
    "anthropic.com",
    "claude.ai",
    "gemini.google.com",
    "bard.google.com",
    "poe.com",
    "character.ai",
    "perplexity.ai",
    "you.com",
    
    # AIå›¾åƒç”Ÿæˆ
    "midjourney.com",
    "stability.ai",
    "stablediffusionweb.com",
    "dall-e.com",
    "firefly.adobe.com",
    "leonardo.ai",
    "playground.ai",
    "craiyon.com",
    
    # AIå¼€å‘å¹³å°
    "huggingface.co",
    "replicate.com",
    "runpod.io",
    "together.ai",
    "cohere.com",
    "ai21.com",
    
    # AIå·¥å…·å’Œåº”ç”¨
    "jasper.ai",
    "copy.ai",
    "writesonic.com",
    "notion.ai",
    "gamma.app",
    "tome.app",
    "beautiful.ai",
    "canva.com",
    
    # AIè§†é¢‘å’ŒéŸ³é¢‘
    "runway.ml",
    "synthesia.io",
    "descript.com",
    "elevenlabs.io",
    "murf.ai",
    
    # AIç ”ç©¶å’Œæ¨¡å‹
    "paperswithcode.com",
    "arxiv.org",
    "kaggle.com",
    "civitai.com",
    
    # AIæœç´¢
    "phind.com",
    "bing.com",  # Copilot
]

def extract_domain_from_url(url: str) -> str:
    """ä»URLæå–ä¸»åŸŸå"""
    if not url:
        return ""
    
    # ç§»é™¤åè®®
    url = re.sub(r'^https?://', '', url)
    # ç§»é™¤è·¯å¾„
    url = url.split('/')[0]
    # ç§»é™¤ç«¯å£
    url = url.split(':')[0]
    
    # æå–ä¸»åŸŸåï¼ˆå»æ‰wwwç­‰å‰ç¼€ï¼‰
    parts = url.split('.')
    if len(parts) >= 2:
        # ä¿ç•™æœ€åä¸¤éƒ¨åˆ†ä½œä¸ºä¸»åŸŸå
        return '.'.join(parts[-2:])
    
    return url

def search_github_ai_projects(max_results: int = 100) -> List[Dict]:
    """æœç´¢GitHubä¸Šçš„çƒ­é—¨AIé¡¹ç›®"""
    projects = []
    
    # å¤šä¸ªæœç´¢å…³é”®è¯
    keywords = [
        "ChatGPT stars:>10000",
        "AI stars:>10000",
        "LLM stars:>10000",
        "stable-diffusion stars:>5000",
        "machine-learning stars:>10000",
        "deep-learning stars:>10000",
    ]
    
    headers = {
        'Accept': 'application/vnd.github.v3+json',
        'User-Agent': 'AI-Projects-Collector'
    }
    
    seen_repos = set()
    
    for keyword in keywords:
        try:
            url = f"https://api.github.com/search/repositories?q={keyword}&sort=stars&order=desc&per_page=30"
            response = requests.get(url, headers=headers, timeout=10)
            
            if response.status_code == 200:
                data = response.json()
                items = data.get('items', [])
                
                for item in items:
                    repo_name = item.get('full_name', '')
                    if repo_name in seen_repos:
                        continue
                    
                    seen_repos.add(repo_name)
                    
                    project = {
                        'name': item.get('name', ''),
                        'full_name': repo_name,
                        'description': item.get('description', ''),
                        'stars': item.get('stargazers_count', 0),
                        'homepage': item.get('homepage', ''),
                        'url': item.get('html_url', ''),
                    }
                    projects.append(project)
                    
                    if len(projects) >= max_results:
                        break
            
            if len(projects) >= max_results:
                break
                
        except Exception as e:
            print(f"Error searching GitHub for '{keyword}': {e}")
            continue
    
    # æŒ‰staræ•°æ’åº
    projects.sort(key=lambda x: x['stars'], reverse=True)
    return projects[:max_results]

def collect_domains(projects: List[Dict]) -> Set[str]:
    """ä»é¡¹ç›®ä¸­æ”¶é›†åŸŸå"""
    domains = set(BUILT_IN_AI_DOMAINS)
    
    for project in projects:
        homepage = project.get('homepage', '')
        if homepage:
            domain = extract_domain_from_url(homepage)
            if domain and '.' in domain:
                # è¿‡æ»¤æ‰æ— æ•ˆåŸŸå
                if domain not in ['github.com', 'github.io', 'localhost']:
                    domains.add(domain)
    
    return domains

def save_data(projects: List[Dict], domains: Set[str], output_file: str):
    """ä¿å­˜æ•°æ®åˆ°JSONæ–‡ä»¶"""
    data = {
        'updated_at': datetime.now().isoformat(),
        'total_projects': len(projects),
        'total_domains': len(domains),
        'domains': sorted(list(domains)),
        'projects': projects[:50],  # åªä¿å­˜å‰50ä¸ªé¡¹ç›®ä¿¡æ¯
    }
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(data, f, indent=2, ensure_ascii=False)
    
    print(f"âœ… Data saved to {output_file}")
    print(f"ğŸ“Š Total domains: {len(domains)}")
    print(f"ğŸ“¦ Total projects: {len(projects)}")

def main():
    print("ğŸš€ Starting AI projects collection...")
    
    # æœç´¢GitHubé¡¹ç›®
    print("ğŸ” Searching GitHub projects...")
    projects = search_github_ai_projects(max_results=100)
    
    # æ”¶é›†åŸŸå
    print("ğŸŒ Collecting domains...")
    domains = collect_domains(projects)
    
    # ä¿å­˜æ•°æ®
    output_file = 'data/ai_projects.json'
    save_data(projects, domains, output_file)
    
    print("âœ¨ Collection completed!")

if __name__ == '__main__':
    main()
